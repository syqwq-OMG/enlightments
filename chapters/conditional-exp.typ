#import "../lib.typ": math-note
#import math-note: *
#import "../utils.typ": *
#show: show-utils
#pagebreak()

== 条件期望

今天上课的时候，概率论老师说了这样一句话：
#quote(block: true)[
  *条件分布，条件期望很深刻，因为条件是人为创造的，就和集合中的辅助线一样。*
]

这不禁引发我的思考......

=== 为什么说它是“辅助线”？

在平面几何中，你面对一个复杂的图形，直接求证很难。但是，如果你人为地画一条线（辅助线），图形突然就被分割成了两个大家熟悉的三角形，原本复杂的关系瞬间变得清晰明了。

在概率论中，全期望公式 (Law of Total Expectation) 就是这条辅助线
$
  E[X] = E[E[X|Y]]
$

- *原本的问题*：直接求 $X$ 的期望 $E[X]$ 可能非常困难，因为 $X$ 的分布太复杂，混杂了各种因素。

- *引入辅助线 ($Y$)*：我们人为引入一个随机变量 $Y$。这个 $Y$ 在原题中可能根本没出现，是我们为了解题“创造”出来的。
- *化整为零*：一旦我们固定了 $Y$ 的值（比如 $Y=y$），在这个条件下，$X$ 的行为可能变得异常简单（变成了简单的三角形）。
- *最后整合*：算出简单的 $E[X|Y]$ 后，再对 $Y$ 求期望，就把问题解决了。

=== 例子们

#problem[
  假设你开了一家商店。每天进店的顾客人数 $N$ 是随机的，均值为 $E[N] = 100$ 人。每位顾客消费的金额 $X_i$ 也是随机的，均值为 $E[X] = 50$ 元。顾客人数和每人的消费金额是独立的。问：你这一天总营业额 $S$ 的期望是多少？
]

#solution()[
  *直接做（很难）*：总金额 $S = X_1 + X_2 +dots.h.c + X_N$。
  请注意，这里的项数 $N$ 本身就是随机的！你不能直接用线性性质拆开，因为你不知道有多少项。如果 $N$ 和 $X$ 的分布很复杂，求 $S$ 的分布简直是噩梦。

  *加“辅助线” (Conditioning)*：我们引入条件：*假设我们知道今天来了 $n$ 个人（即 $N=n$）*。


  + *在条件 $N=n$ 下*：总金额 $S$ 就变成了固定的 $n$ 个随机变量之和：$S = X_1 +dots.h.c + X_n$。这时候求期望就太简单了：
    $
      E[S|N=n] = E[X_1 +dots.h.c + X_n] = n dot E[X]
    $
    这意味着，随机变量 $E[S|N] = N dot E[X]$。
  + *使用全期望公式*：
    $
      E[S] = E[ E[S|N] ] = E[ N dot E[X] ]
    $
    因为 $E[X]$ 是常数，提出来：
    $
      E[S] = E[X] dot E[N] = 50 dot 100 = 5000
    $


  妙处：我们通过引入条件 $N$，把一个变动项数的求和问题，转化为了简单的乘法问题。
]

#problem[
  一只老鼠被困在矿井里，面前有三个门：
  - 门1：通向一条隧道，走 2 小时后回到原点（没出去）。
  - 门2：通向一条隧道，走 3 小时后回到原点（没出去）。
  - 门3：通向一条隧道，走 5 小时后直接逃出矿井。
  
  老鼠每次随机选一个门（概率各 1/3），如果回到原点，它会像失忆一样重新随机选。
  问：老鼠逃出去的平均时间 $E[T]$ 是多少？
]

#solution()[
  过程懒得写了，留作习题（也比较经典哈哈哈）。通过 Conditioning，我们利用了过程的自相似性 (Self-similarity)，把一个无穷级数求和问题变成了一个代数方程。
]

=== 深刻的理解：条件期望是一个“投影”

如果在更高等的数学（希尔伯特空间理论）中看，条件期望 $E[X|Y]$ 实际上是随机变量 $X$ 在由 $Y$ 生成的信息空间上的*正交投影 (Orthogonal Projection)*。
- $X$ 是一个包含所有细节的、起伏不定的函数。

- $E[X|Y]$ 是一个*平滑*后的版本，它去掉了 $X$ 中那些与 $Y$ 无关的“噪音”，只保留了 $X$ 随 $Y$ 变化而变化的“趋势”。
这就像辅助线：辅助线往往是图形的对称轴或高线，它抓住了图形最本质的骨架。条件期望也是抓住了随机变量 $X$ 在条件 $Y$ 下的“骨架”。

#line(length:100%, stroke:luma(210))

具体而言，想象一个巨大的无限维空间，里面的每一个点不是坐标，而是一个随机变量 $X$。

- *向量*：随机变量 $X$。

- 长度（范数）：$norm(X) = sqrt(E[X^(2) ])$。
- 角度（内积）：$iprod(Z,X) = E[X Z]$。如果 $E[X Z] = 0$，我们就说 $X$ 和 $Z$ 是*正交（垂直）*的。
- 距离：$norm(X-Z) = sqrt(E[(X-Z)^(2) ])$。这是“均方误差”。

这是一个 $L^(2)$ 空间。 

现在，假设我们观测到了随机变量 $Y$。
- $Y$ 带来的信息生成了一个*子空间 (Subspace)*，记为 $cal(M)_Y$。

- 在这个子空间里的所有向量，都是 $Y$ 的函数，即形如 $g(Y)$ 的随机变量。
- *直观理解*：$cal(M)_Y$ 就像是三维空间里的一个“平面”。我们只能在这个平面上活动，因为我们只知道 $Y$。

现在，有一个随机变量 $X$（目标），它漂浮在这个平面 $cal(M)_Y$ 的外面（因为 $X$ 包含了一些 $Y$ 无法解释的随机性）。

*任务*：我们要在这个平面 $cal(M)_Y$ 上找到一个点 $Z$，使得 $Z$ 离 $X$ 最近。也就是要最小化“距离”：
$
min_(Z in cal(M)_(Y) ) norm(X-Z)^(2) = min_(g) E[(X-g(Y))^(2) ] 
$

*几何直觉*：在几何学中，从平面外一点 $X$ 到平面 $cal(M)_Y$ 的最短距离，就是从 $X$ 向平面做*垂线*。垂足就是*正交投影*。

根据希尔伯特空间的投影定理，这个最佳逼近点 $Z$ 必须满足：
*误差向量 $(X - Z)$ 必须垂直于平面上的任意向量 $W$*。
（使用变分法，考虑 $J(epsilon) = norm((X-Z)-epsilon W)^(2)$，然后求导可得等价于内积为0） 
用内积公式写出来就是：
$
forall W in cal(M)_(Y) , E[(X - Z) W] = 0 <=>
E[X W] = E[Z W]
$

惊人的事实：数学家发现，满足这个几何性质的 $Z$，正是我们定义的条件期望 $E[X|Y]$！（证明在后面呜呜呜）

所以：*$E[X|Y]$ 就是 $X$ 在 $Y$ 所生成的子空间上的正交投影*。
- 它去掉了 $X$ 中与 $Y$ “垂直”（无关）的噪音部分。

- 它保留了 $X$ 中“平行”（相关）于 $Y$ 的部分。
- 这就是为什么在最小均方误差（MSE）意义下，$E[X|Y]$ 是 $X$ 的最佳估计。

考虑随机变量独立性的一般定义
#definition(title: [独立随机变量])[
  随机变量 $X$ 和 $Y$ 独立，当且仅当
  $
  P(A B) = P(A) dot P(B)
  $
]

这意味着， $P(B) = P(B|A)$，这相当于是 $E[X|Y]=E[X]$。因为如果说条件期望是“投影”，那么独立性就意味着*“投影之后只剩下一个常数中心”*。 

当我们做投影时：
- *一般情况（相关）*：比如 $B$ 和 $A$ 有夹角（相关）。把 $B$ 投影到 $A$ 的轴上，你会得到一个随 $A$ 变化的影子（长短不一）。这就是 $P(B|A)$ 随着 $A$ 变化。

- *独立情况*：$B$ 的“变化部分”完全垂直于 $A$ 的轴。
  - 你把 $B$ 强行投影到 $A$ 的空间上。

  - 因为 $B$ 的波动方向和 $A$ 完全无关（垂直），所以 $B$ 的波动在 $A$ 上的投影是 0。
  - 投影剩下的结果是什么？只剩下 $B$ 的*平均值（重心）*。

如果 $A, B$ 独立：
+ 向量 $bb(1)_B$ 的“变化方向”与向量 $bb(1)_A$ 的空间完全垂直。

+ 如果你站在 $A$ 的角度去看 $B$，你完全看不到 $B$ 的任何“起伏”或“趋势”。
+ 你只能看到 $B$ 的平均大小，也就是 $P(B)$。

=== 一些应用

把条件期望看作正交投影，简直是概率论里的“降维打击”。一旦你戴上这副几何眼镜，很多原本需要繁琐积分证明的性质，瞬间就变成了直观的几何公理。

这里有三个最“妙”的结论，它们从几何角度看是显然的，但在统计和机器学习中威力无穷。

#theorem(title:[重期望公式])[
  假设 $sigma(Z) subset sigma(Y)$，则
  $
  E[E[X|Y]|Z] = E[X|Z]
  $
]
*几何视角*（秒杀）：想象三个空间：
+ *全空间*：包含 $X$ 的大宇宙。

+ *中空间* ($cal(M)_Y$)：由 $Y$ 生成的子空间（比如这是一个平面）。
+ *小空间* ($cal(M)_Z$)：由 $Z$ 生成的子空间。因为 $Z$ 的信息比 $Y$ 少，所以 $cal(M)_Z$ 是 $cal(M)_Y$ 里面的一条直线。

*公式*的含义：
-  $E[X|Y]$：先把 $X$ 投影到*平面* $Y$ 上，得到影子 $X_Y$。

- $E[ dot|Z ]$：再把刚才那个影子 $X_Y$，投影到*直线* $Z$ 上。

（如果 $Z$ 是常数轴 $cal(M)_(0) $，就得到 $E[X] = E[E[X|Y]]$）

*结论*：先把一个点垂直投影到平面上，再把平面上的影子垂直投影到平面内的直线上，*这等价于直接把那个点垂直投影到直线上*！

影子的影子，就是原始物体的影子。这就是为什么内层的 $Y$ 直接“消失”了。

#theorem(title:[全方差公式 (Law of Total Variance)])[
  $
  Var(X) = E[Var(X|Y)] + Var(E[X|Y])
  $

  总方差 = 组内方差的均值 + 组间方差。这是统计学中方差分析 (ANOVA) 的基石。
]

*几何视角（勾股定理）*：我们回顾投影的几何结构。
- 向量 $X$ 是原始变量。
- 向量 $Z = E[X|Y]$ 是投影（最佳预测）。
- 向量 $epsilon= X - E[X|Y]$ 是误差（残差）。

根据正交投影的性质，*预测值 $Z$ 和误差 $epsilon$ 是垂直（正交）的*！

在欧几里得几何中，直角三角形的斜边平方等于两直角边平方和：
$
norm(X)^(2) = norm(Z+epsilon)^(2) = norm(Z)^(2) + norm(epsilon)^(2)
$

(这里我们要考虑中心化后的向量，即减去均值 $E[X]$)

对应到概率论中，长度的平方就是方差：
- 斜边平方 $to$ $Var(X)$ (*总波动*)
- 直角边1平方 $to$ $Var(E[X|Y])$ (*被 $Y$ 解释的波动/信号的能量*)
- 直角边2平方 $to$ $E[Var(X|Y)]$ (*没被 $Y$ 解释的波动/噪音的能量*)

*妙处*：你不需要死记硬背复杂的方差公式，只需要画一个直角三角形：*总风险 = 预测模型能解释的风险 + 预测模型剩下的纯噪音*。

#theorem(title:[已知因子提出])[
  如果 $f(Y)$ 是已知的，它可以从期望中提出：
  $
  E[f(Y)dot X|Y] = f(Y) dot E[X|Y]
  $
]

#let proj = math.op("proj")
*几何视角*（线性性质）：在这个几何空间里，随机变量 $X$ 是向量。而 $f(Y)$ 是什么？因为 $f(Y)$ 是 $sigma(Y)$-可测的，它位于投影的目标子空间 $cal(M)_Y$ 里。在这个子空间的视角下，$f(Y)$ 不像是一个向量，而更像是一个*“标量” (Scalar)* 或者说系数。

这个性质相当于说：投影是一个线性算子。$proj(c dot v) = c dot proj(v)$

虽然 $c$ (即 $f(Y)$) 本身是随机的，但相对于子空间 $cal(M)_Y$ 而言，它的值是确定的（Fixing the condition）。所以它就像常数一样，可以直接提出来。

#line(length:100%, stroke:luma(210))

接下来补一些证明，我也看不懂。

假设我们有概率空间 $(Omega, cal(F), P)$， 
- $X$ 是该空间上的一个随机变量，且满足 $E[abs(X)] < oo$（即 $X in L^1(Omega, cal(F), P)$）。

- $cal(G)$ 是 $cal(F)$ 的一个子 $sigma$-代数（$cal(G) subset.eq cal(F)$）。这意味着 $cal(G)$ 包含的信息比 $cal(F)$ 少（颗粒度更粗）。

我们需要在较小的 $sigma$-代数 $cal(G)$ 上寻找一个随机变量来近似 $X$。为此，我们在 $(Omega, cal(G))$ 上定义一个新的集函数 $nu$：
$
nu(A) = integral_(A) X dd(P), quad forall A in cal(G)
$

注意：
+ 如果 $X >= 0$，则 $nu$ 是 $(Omega, cal(G))$ 上的一个*测度*。
+ 如果 $X$ 取一般值，则 $nu$ 是 $(Omega, cal(G))$ 上的一个*符号测度（Signed Measure）*。

观察 $nu$ 和 $P$（限制在 $cal(G)$ 上，记为 $P|_cal(G)$）的关系：对于任意 $A in cal(G)$，如果 $P(A) = 0$，那么根据积分的性质，必有 $nu(A) = integral_A X dif P = 0$。

这说明 $nu$ 对于 $P|_cal(G)$ 是 绝对连续 的（记作 $nu <<P|_cal(G)$）。

根据 Radon-Nikodym 定理，存在一个唯一的（在 $P$-几乎处处意义下）$cal(G)$-可测函数 $Z$，使得对于任意 $A in cal(G)$：
$
nu(A) = integral_(A) Z dd(P|_cal(G)) = integral_(A) Z dd(P)
$

这个 Radon-Nikodym 导数 $Z = dv(nu, P|_cal(G))$ 就被定义为 $X$ 关于 $cal(G)$ 的条件期望。记作 $E[X|cal(G)]$。

#definition(title:[条件期望])[
  $Y=E[X|cal(G)]$ 是满足以下两个条件的唯一随机变量（几乎处处）
  + *可测性*: $Y$ 是 $cal(G)$-可测的。
  + *局部平均性*: 对于任意 $A in cal(G)$，有 $integral_(A) Y dd(P) = integral_(A) X dd(P)$。 
]

为了讨论“正交投影”，我们需要内积结构，因此我们将讨论范围限制在 $L^2$ 空间（平方可积随机变量空间）。

*几何背景*
- 令 $H = L^2(Omega, cal(F), P)$ 为希尔伯特空间，其内积定义为 $iprod(X,Y) = E[X Y]$。
- 令 $M = L^2(Omega, cal(G), P)$。由于 $cal(G) subset.eq cal(F)$，$M$ 是 $H$ 的一个 *闭子空间*。

*目标*： 证明对于任意 $X in H$，$E[X|cal(G)]$ 是 $X$ 在子空间 $M$ 上的正交投影。即证明 $X - E[X|cal(G)]$ 垂直于子空间 $M$ 中的任意向量。

#proof[
  设 $Y = E[X|cal(G)]$，我们要证明两点：
  + $Y in cal(G)$ （即 $Y$ 在子空间内）。

  + $(X-Y) bot M$ （即误差向量垂直于子空间）。

  根据定义， $Y$ 是 $cal(G)$-可测的，且由 Jensen 不等式 
  $
  E[Y^(2) ] = E[E[X|cal(G)]^(2) ] <= E[E[X^(2) |cal(G)] ] = E[X^(2) ]<oo
  $
  因此 $Y in M$. 
  我们需要证明对于任意的 $Z in M$（即任意 $cal(G)$-可测的平方可积随机变量），都有：
  $
  iprod(X - Y, Z)= 0
  $
  展开有
  $
  iprod(X-Y, Z) = E[(X-E[X|cal(G)]) dot Z] = integral_(Omega) (X - E[X|cal(G)]) dot Z dd(P) 
  $
  根据条件期望的定义， $forall A in cal(G), integral_(A) (X-E[X|cal(G)]) dd(P)=0$，这意味着 $(X - E[X|cal(G)])$ 与 任何 $bb(1)_(A)(A in cal(G))$ 正交。由于 $cal(G)$-可测简单函数是示性函数的线性组合，且简单函数在 $L^(2) (cal(G))$ 中稠密，因此 
  $
  E[(X - E[X|cal(G)]) Z]=iprod(X - Y, Z) = 0, quad forall Z in M
  $
]

由于 $E[X|cal(G) in L^2(cal(G))$ 且误差向量 $X - E[X|cal(G)]$ 垂直于整个子空间 $L^2(cal(G))$，根据希尔伯特空间的投影定理（Projection Theorem），$E[X|cal(G)]$ 正是 $X$ 在 $L^2(cal(G))$ 上的正交投影。